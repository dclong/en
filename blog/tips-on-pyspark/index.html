<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Process Big Data Using PySpark</title>
  <meta name="author" content="Ben Chuanlong Du">

  <link href="https://www.legendu.net/en/atom.xml" type="application/atom+xml" rel="alternate"
        title="Ben Chuanlong Du's Blog Atom Feed" />


  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link href="https://www.legendu.net/en/favicon.png" rel="icon">
  <link href="https://www.legendu.net/en/theme/css/main_2.css" media="screen, projection"
        rel="stylesheet" type="text/css">
  <script src="https://www.legendu.net/en/theme/js/modernizr-2.0.js"></script>
  <script src="https://www.legendu.net/en/theme/js/ender.js"></script>
  <script src="https://www.legendu.net/en/theme/js/octopress.js" type="text/javascript"></script>

  <link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <script data-ad-client="ca-pub-3836424032203369" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="https://www.legendu.net/en/">Ben Chuanlong Du's Blog</a></h1>
    <h2>And let it direct your passion with reason.</h2>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="https://www.legendu.net/en/atom.xml" rel="subscribe-rss">RSS</a></li>
</ul>

<form name="search" action="https://www.bing.com/search" method="get" onSubmit="return build_query()">
  <fieldset role="search">
      <!--
    <input type="hidden" name="q" value="site:https://www.legendu.net/en" />
        -->
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>

<script type = "text/javascript">
    function build_query() {
        var query = document.forms["search"]["q"].value;
        var prefix = "site:https://www.legendu.net/en "
        if (!query.startsWith(prefix)) {
            query = prefix + query;
            document.forms["search"]["q"].value = query;
        }
        return true;
    }
</script>

<ul class="main-navigation">
    <li><a href="https://www.legendu.net">Home</a></li>
    <li><a href="https://www.legendu.net/en">Blog</a></li>
    <li><a href="https://www.legendu.net/en/archives.html">Archives</a></li>
    <li><a href="https://www.legendu.net/pages/about">About</a></li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Process Big Data Using PySpark</h1>
      <p class="meta"><time datetime="2021-04-30T11:49:58-07:00" pubdate>Apr 30, 2021</time></p>
</header>

  <div class="entry-content"><ol>
<li>
<p>PySpark 2.4 and older does not support Python 3.8.
    You have to use Python 3.7 with PySpark 2.4 or older.</p>
</li>
<li>
<p>It can be extremely helpful to run a PySpark application locally to detect possible issues
    before submitting it to the Spark cluster.</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/usr/bin/env bash</span>
<span class="nv">PATH</span><span class="o">=</span>/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin<span class="w"> </span><span class="se">\</span>
/path/to/spark-2.3.1-bin-hadoop2.7/bin/spark-submit<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.yarn.maxAppAttempts<span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.yarn.appMasterEnv.ARROW_PRE_0_15_IPC_FORMAT<span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.executorEnv.ARROW_PRE_0_15_IPC_FORMAT<span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.sql.execution.arrow.enabled<span class="o">=</span>True<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.pyspark.driver.python<span class="o">=</span>.venv/bin/python3<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.pyspark.python<span class="o">=</span>.venv/bin/python3<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>script_to_run.py<span class="w"> </span>--arg1<span class="w"> </span>v1<span class="w"> </span>--arg2<span class="w"> </span>v2
</code></pre></div>

</li>
<li>
<p>You can run PySpark interactively using the <code>pyspark</code> command
  and submit a PySpark job to the cluster using the <code>spark-submit</code> command.
    For more details, 
    please refer to
    <a href="https://spark.apache.org/docs/latest/submitting-applications.html#launching-applications-with-spark-submit">Launching Applications with spark-submit</a>.
    Below is an example of shell script for submitting a PySpark job using <code>spark-submit</code>.</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>

/apache/spark2.3/bin/spark-submit<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--files<span class="w"> </span><span class="s2">&quot;file:///apache/hive/conf/hive-site.xml,file:///apache/hadoop/etc/hadoop/ssl-client.xml,file:///apache/hadoop/etc/hadoop/hdfs-site.xml,file:///apache/hadoop/etc/hadoop/core-site.xml,file:///apache/hadoop/etc/hadoop/federation-mapping.xml&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--master<span class="w"> </span>yarn<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--deploy-mode<span class="w"> </span>cluster<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--queue<span class="w"> </span>YOUR_QUEUE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num-executors<span class="w"> </span><span class="m">200</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--executor-memory<span class="w"> </span>10G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--driver-memory<span class="w"> </span>15G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--executor-cores<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.yarn.maxAppAttempts<span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.dynamicAllocation.enabled<span class="o">=</span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.dynamicAllocation.maxExecutors<span class="o">=</span><span class="m">1000</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.network.timeout<span class="o">=</span>300s<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.executor.memoryOverhead<span class="o">=</span>2G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.yarn.appMasterEnv.ARROW_PRE_0_15_IPC_FORMAT<span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.executorEnv.ARROW_PRE_0_15_IPC_FORMAT<span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.sql.execution.arrow.enabled<span class="o">=</span>True<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.executor.extraJavaOptions<span class="o">=</span>-XX:MaxDirectMemorySize<span class="o">=</span>8G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.pyspark.driver.python<span class="o">=</span>/usr/share/anaconda3/bin/python<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.pyspark.python<span class="o">=</span>/usr/share/anaconda3/bin/python<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>/path/to/_pyspark.py
</code></pre></div>

<p>If you use a portable Python environment named <code>env.tar.gz</code>,
you can submit a PySpark application using the following shell script.</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>

/apache/spark2.3/bin/spark-submit<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--files<span class="w"> </span><span class="s2">&quot;file:///apache/hive/conf/hive-site.xml,file:///apache/hadoop/etc/hadoop/ssl-client.xml,file:///apache/hadoop/etc/hadoop/hdfs-site.xml,file:///apache/hadoop/etc/hadoop/core-site.xml,file:///apache/hadoop/etc/hadoop/federation-mapping.xml&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--master<span class="w"> </span>yarn<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--deploy-mode<span class="w"> </span>cluster<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--queue<span class="w"> </span>YOUR_QUEUE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num-executors<span class="w"> </span><span class="m">200</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--executor-memory<span class="w"> </span>10G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--driver-memory<span class="w"> </span>15G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--executor-cores<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.yarn.maxAppAttempts<span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.dynamicAllocation.enabled<span class="o">=</span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.dynamicAllocation.maxExecutors<span class="o">=</span><span class="m">1000</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.network.timeout<span class="o">=</span>300s<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.executor.memoryOverhead<span class="o">=</span>2G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.yarn.appMasterEnv.ARROW_PRE_0_15_IPC_FORMAT<span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.executorEnv.ARROW_PRE_0_15_IPC_FORMAT<span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.sql.execution.arrow.enabled<span class="o">=</span>True<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.executor.extraJavaOptions<span class="o">=</span>-XX:MaxDirectMemorySize<span class="o">=</span>8G<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.pyspark.driver.python<span class="o">=</span>./env/bin/python<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--conf<span class="w"> </span>spark.pyspark.python<span class="o">=</span>./env/bin/python<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--archives<span class="w"> </span>env.tar.gz#env<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">$1</span>
</code></pre></div>

<p>And below is a simple example of <code>_pyspark.py</code>.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;Test PySpark&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">enableHiveSupport</span><span class="p">()</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="n">sql</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        SELECT * </span>
<span class="s2">        FROM some_table </span>
<span class="s2">        TableSample (100000 Rows)</span>
<span class="s2">    &quot;&quot;&quot;</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="n">sql</span><span class="p">)</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s2">&quot;overwrite&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;output&quot;</span><span class="p">)</span>
</code></pre></div>

<p>Notice that I have prefixed an underscore to the name of the file.
This is a simple but useful trick to avoid unintentional module conflictions in Python. 
For example, 
if you name your PySpark script <code>pyspark.py</code>,
your PySpark application will fail to work 
as your script <code>pyspark.py</code> is loaded as a module named <code>pyspark</code> 
which hides the official <code>pyspark</code> module in Python.
It is suggestion that adopt the trick of "prefixing an underscore to file names"
when submitting a PySpark job.</p>
</li>
<li>
<p>If you have multiple versions of Spark installed,
    the exported Spark environment variables might intervent with each other 
    and cause some of them fail to work.
    For example, 
    if you have both a cluster version of Spark and a local version of Spark installed,
    you might failed to submit Spark applications using <code>spark-submit</code>.
    One simple fix to this problem is 
    to manually configure a right version of the <code>PATH</code> environemnt varible 
    before you invoke the command <code>spark-submit</code> of the local version of Spark.</p>
<div class="highlight"><pre><span></span><code><span class="nv">PATH</span><span class="o">=</span>/bin:/sbin:/usr/bin:/usr/sbin<span class="w"> </span>/path/to/spark-submit<span class="w"> </span>...
</code></pre></div>

<p>As a matter of fact,
this is the way to fix most PATH related issues in Linux/Unix.</p>
</li>
<li>
<p>PySpark does not support converting <code>$"col"</code> to a Column implicitly. 
    However, 
    the function <code>pyspark.sql.functions.col</code> works the same as in Spark.</p>
</li>
<li>
<p><a href="https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html#pandas-udfs-aka-vectorized-udfs">Pandas UDFs</a>
    is preferred to UDFs.
    Please refer to 
    <a href="http://www.legendu.net/en/blog/pyspark-udf/">User-defined Function (UDF) in PySpark</a>
    for more discussions.</p>
</li>
<li>
<p><a href="https://github.com/zero323/pyspark-stubs">pyspark-stubs</a>
    can be leveraged for static type checking for PySpark project.</p>
</li>
</ol>
<h2 id="use-pyspark-in-jupyterlab-notebooks">Use PySpark in Jupyter/Lab Notebooks</h2>
<ol>
<li>
<p>The trick is to use the Python library <code>findspark</code> to find and initiate Spark for use in notebook. </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">findspark</span>
<span class="n">findspark</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="s2">&quot;/opt/&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s2">&quot;spark-*&quot;</span><span class="p">))))</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span><span class="p">,</span> <span class="n">DataFrame</span>
<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;PySpark_Notebook&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">enableHiveSupport</span><span class="p">()</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</code></pre></div>

<p>Notice that the following 2 lines of code must be before any code that involving Spark.
The example here might seems obvious, 
however,
it might not so obvious if you import a module which creates a SparkSession object.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">findspark</span>
<span class="n">findspark</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="s2">&quot;/opt/&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s2">&quot;spark-*&quot;</span><span class="p">))))</span>
</code></pre></div>

</li>
<li>
<p>When working with relatively large data in a local version of Spark in Jupyter/Lab notebook,
    you might easily encounter OOM errors. 
    The trick to increase the driver memory using the option <code>.config("spark.driver.memory", "50g")</code>.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">findspark</span>
<span class="n">findspark</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="s2">&quot;/opt/&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s2">&quot;spark-*&quot;</span><span class="p">))))</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span><span class="p">,</span> <span class="n">DataFrame</span>
<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;PySpark_Notebook&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s2">&quot;local[*]&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.driver.memory&quot;</span><span class="p">,</span> <span class="s2">&quot;50g&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">enableHiveSupport</span><span class="p">()</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</code></pre></div>

</li>
<li>
<p>The command-line option <code>--jars</code> is equivalent to <code>spark.jars</code> when you use <code>--conf</code>.
    This means that you can use <code>.config("spark.jars", "/path/to/file.jar")</code> 
    to add JARs to a Spark/PySpark application in Jupyter/Lab notebook.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">findspark</span>
<span class="n">findspark</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="s2">&quot;/opt/&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s2">&quot;spark-*&quot;</span><span class="p">))))</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span><span class="p">,</span> <span class="n">DataFrame</span>
<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;PySpark_Notebook&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.jars&quot;</span><span class="p">,</span> <span class="s2">&quot;/path/to/file.jar&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.driver.memory&quot;</span><span class="p">,</span> <span class="s2">&quot;50g&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">enableHiveSupport</span><span class="p">()</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</code></pre></div>

</li>
<li>
<p>If you want to leverage a Spark cluster in a Jupyter/Lab notebook,
    there are a few things (Hadoop queue, driver IP and driver port) you need to configure.
    Below is an illustration.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">socket</span>
<span class="kn">import</span> <span class="nn">findspark</span>
<span class="n">findspark</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="s2">&quot;/apache/spark2.3&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span><span class="p">,</span> <span class="n">DataFrame</span>
<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s2">&quot;yarn&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;PySpark_Cluster&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.yarn.queue&quot;</span><span class="p">,</span> <span class="s2">&quot;your-hadoop-queue&gt;&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.driver.host&quot;</span><span class="p">,</span> <span class="n">socket</span><span class="o">.</span><span class="n">gethostbyname</span><span class="p">(</span><span class="n">socket</span><span class="o">.</span><span class="n">gethostname</span><span class="p">()))</span> \
    <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.driver.port&quot;</span><span class="p">,</span> <span class="s2">&quot;30202&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">enableHiveSupport</span><span class="p">()</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</code></pre></div>

<p>Notice that the SparkContext object will be expired after inactive for a while.
It won't help if you run the above code again in notebook
because a SparkSession (even if it is invalid any more due to expiration of the underlying SparkContext) has already been created.
You can of course restart the Python kernel and then create a new SparkSession.
However, 
you will lose all Python objects in the notebook by doing this.
A better alternative is to manually stop the SparkSession by calling <code>spark.stop()</code> 
and then run the above code in notebook again.
This way, the Python objects (e.g., pandas DataFrame you have created)
will still be alive for you to use.</p>
</li>
<li>
<p>When you run Spark or PySpark in a Jupyter/Lab notebook, 
    it is recommended that you show ERROR messages only. 
    Otherwise, 
    there might be too much logging information polluting your notebook. 
    You can set the log level of Spark to <code>ERROR</code> using the following line of code.</p>
<div class="highlight"><pre><span></span><code><span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">setLogLevel</span><span class="p">(</span><span class="s2">&quot;ERROR&quot;</span><span class="p">)</span>
</code></pre></div>

<p>For more details, 
please refer to 
<a href="http://www.legendu.net/misc/blog/configure-log4j-for-spark/">Configure Log4J for Spark</a>
.</p>
</li>
</ol>
<h2 id="python-dependencies-for-pyspark">Python Dependencies for PySpark</h2>
<p>Listed below are several ways to handle dependencies for PySpark.</p>
<ol>
<li><a href="http://www.legendu.net/en/blog/packaging-Python-Dependencies-for-PySpark-Using-python-build-standalone">Build a portable Python environment using python-build-standalone</a></li>
<li><a href="http://www.legendu.net/en/blog/packaging-python-dependencies-for-pyspark-using-conda-pack">Build a portable Python environment using conda-pack</a></li>
<li><a href="http://www.legendu.net/misc/blog/packaging-python-dependencies-for-pyspark-using-pex">Packaging Python dependencies using pex</a></li>
</ol>
<p>Building a portable Python environment using python-build-standalone is the recommended approach.</p>
<h2 id="references">References</h2>
<ul>
<li>
<p><a href="https://spark.apache.org/docs/latest/api/python/reference/">PySpark API Reference</a></p>
</li>
<li>
<p><a href="https://developerzen.com/best-practices-writing-production-grade-pyspark-jobs-cb688ac4d20f">Best Practices Writing Production-Grade PySpark Jobs</a></p>
</li>
<li>
<p><a href="https://medium.com/criteo-labs/packaging-code-with-pex-a-pyspark-example-9057f9f144f3">Packaging code with PEX â€” a PySpark example</a></p>
</li>
<li>
<p><a href="https://github.com/awesome-spark/awesome-spark">awesome-spark</a></p>
</li>
</ul></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">Benjamin Du</span>
  </span>
<time datetime="2021-04-30T11:49:58-07:00" pubdate>Apr 30, 2021</time>  <span class="categories">
    <a class="category" href="https://www.legendu.net/en/tag/programming.html">programming</a>
    <a class="category" href="https://www.legendu.net/en/tag/pyspark.html">PySpark</a>
    <a class="category" href="https://www.legendu.net/en/tag/python.html">Python</a>
    <a class="category" href="https://www.legendu.net/en/tag/spark.html">Spark</a>
    <a class="category" href="https://www.legendu.net/en/tag/tips.html">tips</a>
    <a class="category" href="https://www.legendu.net/en/tag/hpc.html">HPC</a>
    <a class="category" href="https://www.legendu.net/en/tag/high-performance-computing.html">high performance computing</a>
  </span>
</p>    </footer>
  </article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div>
  </section>
</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="https://www.legendu.net/en/blog/find-tips/">Tips on the find command in Linux</a>
      </li>
      <li class="post">
          <a href="https://www.legendu.net/en/blog/hands-on-dict-python/">Hands on dict in Python</a>
      </li>
      <li class="post">
          <a href="https://www.legendu.net/en/blog/install-python-in-mac/">Install Python in macOS</a>
      </li>
      <li class="post">
          <a href="https://www.legendu.net/en/blog/my-docker-images/">My Docker Images</a>
      </li>
      <li class="post">
          <a href="https://www.legendu.net/en/blog/pdftk-examples/">Use pdftk to Manipulating PDF Files</a>
      </li>
    </ul>
  </section>
  <section>
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="https://www.legendu.net/en/category/ai.html">AI</a></li>
        <li><a href="https://www.legendu.net/en/category/career.html">Career</a></li>
        <li><a href="https://www.legendu.net/en/category/computer-science.html">Computer Science</a></li>
        <li><a href="https://www.legendu.net/en/category/fun-problems.html">Fun Problems</a></li>
        <li><a href="https://www.legendu.net/en/category/internet.html">Internet</a></li>
        <li><a href="https://www.legendu.net/en/category/os.html">OS</a></li>
        <li><a href="https://www.legendu.net/en/category/research.html">Research</a></li>
        <li><a href="https://www.legendu.net/en/category/software.html">Software</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
  <ul class="tagcloud">
  </ul>
  </section>



  <section>
    <h1>GitHub Repos</h1>
    <ul id="gh_repos">
      <li class="loading">Status updating...</li>
    </ul>
      <a href="https://github.com/legendu-net">@legendu-net</a> on GitHub
    <script type="text/javascript">
      $.domReady(function(){
          if (!window.jXHR){
              var jxhr = document.createElement('script');
              jxhr.type = 'text/javascript';
              jxhr.src = 'https://www.legendu.net/en/theme/js/jXHR.js';
              var s = document.getElementsByTagName('script')[0];
              s.parentNode.insertBefore(jxhr, s);
          }

          github.showRepos({
              user: 'legendu-net',
              count: 5,
              skip_forks: true,
              target: '#gh_repos'
          });
      });
    </script>
    <script src="https://www.legendu.net/en/theme/js/github.js" type="text/javascript"> </script>
  </section>

    <section>
        <h1>Social</h2>
        <ul>
            <li><a href="https://www.linkedin.com/in/ben-chuanlong-du-1239b221/" target="_blank">LinkedIn</a></li>
            <li><a href="https://hub.docker.com/u/dclong" target="_blank">Docker Hub</a></li>
            <li><a href="https://stackoverflow.com/users/7808204/benjamin-du?tab=profile" target="_blank">Stack Overflow</a></li>
            <li><a href="https://twitter.com/longendu" target="_blank">Twitter</a></li>
        <!--
            <li><a href="https://www.legendu.net/en/" type="application/rss+xml" rel="alternate">RSS</a></li>
            -->
        </ul>
    </section>
</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2013 - Ben Chuanlong Du -
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3STS9BVPF6"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-3STS9BVPF6');
</script>
	<script type="text/javascript">
	  var disqus_shortname = 'dclong';
          var disqus_identifier = '/blog/tips-on-pyspark/';
          var disqus_url = 'https://www.legendu.net/en/blog/tips-on-pyspark/';
          var disqus_title = 'Process Big Data Using PySpark';
	  (function() {
	    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	    dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
	    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	   })();
	</script>
</body>
</html>